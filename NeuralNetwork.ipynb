{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubhra-1/Group9-MI_classification/blob/main/NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRGn5OfeqkQK"
      },
      "outputs": [],
      "source": [
        "# Setting Seed value\n",
        "seed_value= 0\n",
        "\n",
        "# 1. Setting the `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Setting the `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Setting the `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Setting the `tensorflow` pseudo-random generator at a fixed value\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(seed_value)\n",
        "\n",
        "# 5. Configuring a new global `tensorflow` session\n",
        "from keras import backend as K\n",
        "\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "tf.compat.v1.keras.backend.set_session(sess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk8NIXYoqkQN"
      },
      "source": [
        "# **Neural Network Models**\n",
        "1. Regular Neural Net\n",
        "2. CNN\n",
        "3. Transfer Learning with ResNet50\n",
        "4. Autoencoder\n",
        "5. Mobilenet\n",
        "6. Inceptionnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9aSW0dvqkQP"
      },
      "outputs": [],
      "source": [
        "# System imports\n",
        "import re\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upMvhJUGqkQQ"
      },
      "outputs": [],
      "source": [
        "# Data science imports\n",
        "import pandas as pd \n",
        "import numpy as np "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOT7qNEtqkQQ"
      },
      "outputs": [],
      "source": [
        "# Visualization imports\n",
        "import matplotlib.pyplot as plt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAJ1UkG_qkQR"
      },
      "outputs": [],
      "source": [
        "# sklearn imports\n",
        "import sklearn.model_selection \n",
        "import sklearn.linear_model\n",
        "import sklearn.ensemble\n",
        "import sklearn.svm\n",
        "import sklearn.discriminant_analysis\n",
        "import sklearn.metrics\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.exceptions import ConvergenceWarning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcU_QA-9qkQR"
      },
      "outputs": [],
      "source": [
        "# Tensorflow imports\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Dropout, MaxPooling2D, Flatten, Input\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.applications import ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9oddHvwqkQS"
      },
      "outputs": [],
      "source": [
        "# Visualization Imports\n",
        "import visualkeras\n",
        "from tensorflow.keras import layers, Sequential\n",
        "from PIL import ImageFont\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils.vis_utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiO-56TQqkQS"
      },
      "outputs": [],
      "source": [
        "# Helper function to compute metrics\n",
        "\n",
        "def get_f1(y_true, y_pred): \n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "\n",
        "    true_negatives=int(len(y_pred)) - ((int(possible_positives) + int(predicted_positives))-int(true_positives))\n",
        "    return(f1_val)\n",
        "\n",
        "def acc(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "\n",
        "    true_negatives=int(len(y_pred)) - ((int(possible_positives) + int(predicted_positives))-int(true_positives))\n",
        "    accuracy = (tf.cast(true_positives, tf.float32) + tf.cast(true_negatives, tf.float32)) / tf.cast(len(y_pred), tf.float32)\n",
        "    return(accuracy)\n",
        "\n",
        "def prec(y_true, y_pred): \n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "\n",
        "    true_negatives=int(len(y_pred)) - ((int(possible_positives) + int(predicted_positives))-int(true_positives))\n",
        "    return(precision)\n",
        "\n",
        "def rec(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "\n",
        "    true_negatives=int(len(y_pred)) - ((int(possible_positives) + int(predicted_positives))-int(true_positives))\n",
        "    return(recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UemiEwSnqkQT"
      },
      "outputs": [],
      "source": [
        "# Helper class to train sklearn gridsearchcv models & report metrics\n",
        "class gridsearchcv_model:\n",
        "    def __init__(self, model, X_train, Y_train, X_val, Y_val, parameter_matrix={}, is_classification=False, cv=4):\n",
        "        self.is_classification = is_classification\n",
        "        self.train_model(model, X_train, Y_train, X_val, Y_val, parameter_matrix, cv)\n",
        "        \n",
        "    # Trains model using a training set and predicts a validation set\n",
        "    def train_model(self, model, X_train, Y_train, X_val, Y_val, parameter_matrix={}, cv=4):\n",
        "        ml_model = sklearn.model_selection.GridSearchCV(model, parameter_matrix, cv=cv, scoring='f1')\n",
        "     \n",
        "        ml_model.fit(X_train, Y_train)\n",
        "        \n",
        "        self.model = ml_model.best_estimator_\n",
        "        self.name = re.compile(\"(.*?)\\s*\\(\").match(str(self.model)).group(1)\n",
        "        \n",
        "        self.train = {'name': 'train'}\n",
        "        self.val = {'name': 'val'}\n",
        "        \n",
        "        self.calculate_error(self.train, X_train, Y_train, self.train['name'])\n",
        "        self.calculate_error(self.val, X_val, Y_val, self.val['name'])\n",
        "        \n",
        "        return ml_model\n",
        "    \n",
        "    def calculate_error(self, var, X_set, Y_set, name):\n",
        "        var['name'] = name\n",
        "        var['predictions'] = self.model.predict(X_set)\n",
        "        \n",
        "        var['f1_score'] = sklearn.metrics.f1_score(Y_set, var['predictions'])\n",
        "        var['accuracy'] = sklearn.metrics.accuracy_score(Y_set, var['predictions'])\n",
        "        var['precision'] = sklearn.metrics.precision_score(Y_set, var['predictions'])\n",
        "        var['recall'] = sklearn.metrics.recall_score(Y_set, var['predictions'])\n",
        "        \n",
        "        self.print_error(var)\n",
        "        \n",
        "    # Prints error metrics\n",
        "    def print_error(self, var):\n",
        "        print(self.name + ' ('+ var['name'] + ')')\n",
        "        \n",
        "        print(\"Accuracy: %0.4f\" % var['accuracy'])\n",
        "        print(\"f1_score: %0.4f\" % var['f1_score'])\n",
        "        print(\"precision: %0.4f\" % var['precision'])\n",
        "        print(\"recall: %0.4f\" % var['recall'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8NWnsA2qkQU"
      },
      "source": [
        "## Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKkpUyTMqkQV"
      },
      "outputs": [],
      "source": [
        "# Load train and test data into dataframes\n",
        "df_train_original = pd.read_pickle (\"data/epoched_train.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boXihjivqkQV"
      },
      "outputs": [],
      "source": [
        "# Create column 'pid' which is the patient ID 1 through 9\n",
        "df_train_original['pid'] = [int(df_train_original['patient_id'][x][2]) for x in range(len(df_train_original))]\n",
        "\n",
        "# Create column 'trial_id' which is the trial 1 through 3\n",
        "df_train_original['trial_id'] = [int(df_train_original['patient_id'][x][-2]) for x in range(len(df_train_original))]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE8-lnwMqkQV"
      },
      "outputs": [],
      "source": [
        "# Dividing the dataset into training, validation and test set \n",
        "\n",
        "df_train = df_train_original[df_train_original['trial_id']!=3]\n",
        "df_train = df_train_original.reindex(np.random.permutation(df_train_original.index)).reset_index(drop=True)\n",
        "\n",
        "df_val_test = df_train_original[df_train_original['trial_id']==3]\n",
        "df_val_test = df_val_test.reindex(np.random.permutation(df_val_test.index)).reset_index(drop=True)\n",
        "\n",
        "df_val = df_val_test[0:int(len(df_val_test)/2)]\n",
        "df_test = df_val_test[int(len(df_val_test)/2):len(df_val_test)]\n",
        "df_train_val = df_train.append(df_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIM1_vR0qkQW"
      },
      "source": [
        "## Preparation of Data & Adding Gaussian Noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv5vhFxYqkQW"
      },
      "outputs": [],
      "source": [
        "# Augment training data by adding Gaussian noise (to avoid overfitting)\n",
        "for _ in range(1): \n",
        "    df_train_augment = df_train.copy()\n",
        "    for x in ['C3', 'Cz', 'C4', 'EOG:ch01', 'EOG:ch02','EOG:ch03']:\n",
        "        df_train_augment[x] += np.random.normal(0,1)\n",
        "\n",
        "    df_train = pd.concat([df_train, df_train_augment])\n",
        "\n",
        "\n",
        "for _ in range(1): \n",
        "    df_train_val_augment = df_train_val.copy()\n",
        "    for x in ['C3', 'Cz', 'C4', 'EOG:ch01', 'EOG:ch02','EOG:ch03']:\n",
        "        df_train_val_augment[x] += np.random.normal(0,1)\n",
        "\n",
        "    df_train_val = pd.concat([df_train_val, df_train_val_augment])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRCZ-vZWqkQW"
      },
      "outputs": [],
      "source": [
        "# Prepare data for training across all subjects\n",
        "y_train = df_train[\"event_type\"].values.astype(float)\n",
        "y_val = df_val[\"event_type\"].values.astype(float)\n",
        "y_train_val = df_train_val[\"event_type\"].values.astype(float)\n",
        "y_test = df_test[\"event_type\"].values.astype(float)\n",
        "\n",
        "X_train = df_train.drop([\"patient_id\", \"start_time\", \"event_type\", \"pid\", \"trial_id\"], axis=1)\n",
        "X_val = df_val.drop([\"patient_id\", \"start_time\", \"event_type\", \"pid\", \"trial_id\"], axis=1)\n",
        "X_train_val = df_train_val.drop([\"patient_id\", \"start_time\", \"event_type\", \"pid\", \"trial_id\"], axis=1)\n",
        "X_test = df_test.drop([\"patient_id\", \"start_time\", \"event_type\",\"pid\", \"trial_id\"], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXlqM4NIqkQX"
      },
      "source": [
        "## **1. Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDfBVmGQqkQX"
      },
      "outputs": [],
      "source": [
        "# Concatenate the data for sklearn & neural net models\n",
        "x_train_nn = np.array(X_train.apply(lambda x:np.concatenate(x), axis=1).values.tolist())\n",
        "x_val_nn = np.array(X_val.apply(lambda x:np.concatenate(x), axis=1).values.tolist())\n",
        "x_train_val_nn = np.array(X_train_val.apply(lambda x:np.concatenate(x), axis=1).values.tolist())\n",
        "x_test_nn = np.array(X_test.apply(lambda x:np.concatenate(x), axis=1).values.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGYyl10OqkQX"
      },
      "outputs": [],
      "source": [
        "# Regular neural net\n",
        "neural_network = keras.Sequential([Dense(512, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.0001)),\n",
        "                                   Dropout(0.2),\n",
        "                                   Dense(32, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.0001)),\n",
        "                                   Dropout(0.2),\n",
        "                                   Dense(1, activation=\"sigmoid\")])\n",
        "\n",
        "neural_network.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=[get_f1, acc, prec, rec])\n",
        "\n",
        "history_nn = neural_network.fit(x_train_val_nn, y_train_val, epochs=25, batch_size=64, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XWoZkT8qkQX"
      },
      "outputs": [],
      "source": [
        "neural_network.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMx7vIG-qkQY"
      },
      "outputs": [],
      "source": [
        "# Evaluation on test set\n",
        "neural_network.evaluate(x_test_nn, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOWlKmmbqkQY"
      },
      "source": [
        "## **2. CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8ht8XK5qkQY"
      },
      "outputs": [],
      "source": [
        "# Stack the data for CNN models\n",
        "x_train_cnn = np.array(X_train.apply(lambda x:np.stack(x, axis=-1), axis=1).values.tolist())\n",
        "x_train_cnn = x_train_cnn.reshape(list(x_train_cnn.shape)+[1])\n",
        "\n",
        "x_train_val_cnn = np.array(X_train_val.apply(lambda x:np.stack(x, axis=-1), axis=1).values.tolist())\n",
        "x_train_val_cnn = x_train_val_cnn.reshape(list(x_train_val_cnn.shape)+[1])\n",
        "\n",
        "x_test_cnn = np.array(X_val.apply(lambda x:np.stack(x, axis=-1), axis=1).values.tolist())\n",
        "x_test_cnn = x_test_cnn.reshape(list(x_test_cnn.shape)+[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0KDzgKTqkQY"
      },
      "outputs": [],
      "source": [
        "cnn = keras.Sequential([Conv2D(32, (3, 3), activation=\"relu\", input_shape=(1000, 6, 1)),\n",
        "                        Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "                        Flatten(),\n",
        "                        Dense(256, activation=\"relu\"),\n",
        "                        Dropout(0.2),\n",
        "                        Dense(128, activation=\"relu\"),\n",
        "                        Dense(1, activation=\"sigmoid\")])\n",
        "\n",
        "cnn.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=[get_f1, acc, prec, rec])\n",
        "\n",
        "history_cnn = cnn.fit(x_train_val_cnn, y_train_val, epochs=15, batch_size=64, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5yx4lURqkQY"
      },
      "outputs": [],
      "source": [
        "cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twD7umH1qkQY"
      },
      "outputs": [],
      "source": [
        "# Evaluation on test set\n",
        "cnn.evaluate(x_test_cnn, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAm5kCRhqkQY"
      },
      "source": [
        "## **3. Resnet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDCdaNT5qkQY"
      },
      "outputs": [],
      "source": [
        "# Reshape data for transfer learning models\n",
        "x_train_transfer = x_train_cnn.reshape((len(x_train_cnn), 50, 40, 3))\n",
        "x_train_val_transfer = x_train_val_cnn.reshape((len(x_train_val_cnn), 50, 40, 3))\n",
        "x_test_transfer = x_test_cnn.reshape((len(x_test_cnn), 50, 40, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmXgns8bqkQZ"
      },
      "outputs": [],
      "source": [
        "resnet = keras.Sequential([ResNet50(include_top=False, weights=None, input_shape=(50,40,3)),\n",
        "                                     Flatten(),\n",
        "                                     Dense(1, activation=\"sigmoid\")])\n",
        "\n",
        "opt = Adam(lr=0.001)\n",
        "\n",
        "resnet.compile(optimizer=opt, loss='binary_crossentropy', metrics=[get_f1, acc, prec, rec])\n",
        "\n",
        "history_rn = resnet.fit(x_train_val_transfer, y_train_val, epochs=25, batch_size=64, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhuj15CXqkQZ"
      },
      "outputs": [],
      "source": [
        "print(resnet.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYiH_8s3qkQZ"
      },
      "outputs": [],
      "source": [
        "# Evaluation on test set\n",
        "resnet.evaluate(x_test_transfer, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZOy95rKqkQZ"
      },
      "source": [
        "## **5. MobileNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Egyh0agCqkQZ"
      },
      "outputs": [],
      "source": [
        "x_train_mn = x_train_nn.reshape([7360,75,80,1])\n",
        "x_train_val_mn = x_train_val_nn.reshape([11760,75,80,1])    \n",
        "x_val_mn = x_val_nn.reshape([720,75,80,1])\n",
        "x_test_mn = x_test_nn.reshape([720,75,80,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjqPTGc7qkQZ"
      },
      "outputs": [],
      "source": [
        "mobile = keras.applications.mobilenet.MobileNet(input_shape=(75, 80, 1),\n",
        "    weights=None,\n",
        "    classes=1\n",
        "    )\n",
        "\n",
        "mobile.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=[get_f1, acc, prec, rec])\n",
        "history_mn = mobile.fit(x_train_val_mn, y_train_val, epochs=50, batch_size=64, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koEMYDDlqkQZ"
      },
      "outputs": [],
      "source": [
        "mobile.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2c8LK7nqkQZ"
      },
      "outputs": [],
      "source": [
        "# Evaluation on test set\n",
        "mobile.evaluate(x_test_mn, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NcoWDXwqkQZ"
      },
      "source": [
        "## **6. Inceptionnet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCkE3uoZqkQa"
      },
      "outputs": [],
      "source": [
        "inception = keras.applications.InceptionV3(\n",
        "    include_top=True,\n",
        "    weights=None,\n",
        "    input_tensor=None,\n",
        "    input_shape=(75, 80, 1),\n",
        "    pooling=None,\n",
        "    classes=1,\n",
        "    classifier_activation=\"softmax\"\n",
        "    )\n",
        "\n",
        "inception.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=[get_f1, acc, prec, rec])\n",
        "history_in = inception.fit(x_train_val_mn, y_train_val, epochs=50, batch_size=64, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s__pO6r9qkQa"
      },
      "outputs": [],
      "source": [
        "inception.summmary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drX-BW1QqkQa"
      },
      "outputs": [],
      "source": [
        "# Evaluation on test set\n",
        "inception.evaluate(x_test_mn, y_test)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "68335a7dc94bdb769d969136a2852d23ad5b822a797b0478900dd794519e60ae"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}